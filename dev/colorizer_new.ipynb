{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, InputLayer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizeing images\n",
    "#f = r'../256_ObjectCategories/input/'\n",
    "#//for file in os.listdir(f):\n",
    "#    f_img = f+\"/\"+file\n",
    "#   img = Image.open(f_img)\n",
    "#    img = img.resize((256,256))\n",
    "#    img.save(f_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get images\n",
    "#Images_new/Train/\n",
    "images = []\n",
    "for filename in os.listdir('../256_ObjectCategories/input/'):\n",
    "    images.append(img_to_array(load_img('../256_ObjectCategories/input/' + filename)))\n",
    "images = np.array(images, dtype=float)\n",
    "\n",
    "# Set up train and test data\n",
    "split = int(0.95*len(images))\n",
    "images_train = images[:split]\n",
    "\n",
    "# 1.0/255 -> 24 bit RGB color, [0-255] for each color channel -> 16.7m combinations -> human can recognize 2-10m \n",
    "images_train = 1.0/255*images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(256, 256, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 55s 2s/step - loss: 0.1791 - accuracy: 0.5544\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0132 - accuracy: 0.6201\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0150 - accuracy: 0.6091\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0131 - accuracy: 0.6186\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0142 - accuracy: 0.6248\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0264 - accuracy: 0.6112\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0140 - accuracy: 0.6253\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0134 - accuracy: 0.6348\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0140 - accuracy: 0.6200\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0139 - accuracy: 0.6168\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0222 - accuracy: 0.6261\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0148 - accuracy: 0.6085\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0136 - accuracy: 0.6165\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0133 - accuracy: 0.5980\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0155 - accuracy: 0.6279\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0139 - accuracy: 0.6262\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0141 - accuracy: 0.6165\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0136 - accuracy: 0.6072\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0159 - accuracy: 0.6124\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0140 - accuracy: 0.6139\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0138 - accuracy: 0.6311\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0162 - accuracy: 0.6184\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0134 - accuracy: 0.6122\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0127 - accuracy: 0.6178\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0149 - accuracy: 0.6293\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0144 - accuracy: 0.5885\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0135 - accuracy: 0.6288\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0133 - accuracy: 0.6220\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0139 - accuracy: 0.6155\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0139 - accuracy: 0.6022\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0131 - accuracy: 0.6018\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0140 - accuracy: 0.6096\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0138 - accuracy: 0.5782\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0130 - accuracy: 0.5996\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0135 - accuracy: 0.5712\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0133 - accuracy: 0.6044\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0123 - accuracy: 0.5994\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0141 - accuracy: 0.5912\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0137 - accuracy: 0.5873\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0130 - accuracy: 0.5742\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0132 - accuracy: 0.6052\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0139 - accuracy: 0.5915\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0120 - accuracy: 0.5926\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0142 - accuracy: 0.6176\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0130 - accuracy: 0.6080\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0136 - accuracy: 0.5909\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0132 - accuracy: 0.6190\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0124 - accuracy: 0.5928\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0131 - accuracy: 0.5982\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0134 - accuracy: 0.5796\n"
     ]
    }
   ],
   "source": [
    "# Image datagenerator, adjust setting for img\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Generate training data\n",
    "batch_size = 29 \n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(images_train, batch_size=batch_size):\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        # extract b/w layer for x_batch, two color layers for y_batch\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)\n",
    "\n",
    "\n",
    "trainedmodel = model.fit_generator(image_a_b_gen(batch_size), epochs=50, steps_per_epoch=33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(\"modelNew_resized2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"modelNew_resized2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 4s 41ms/step\n",
      "[0.010830978009228906, 0.7158740162849426]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test images \n",
    "# Lab color, L -> grayscale layer, a b -> color layers (green-red, blue-yellow)\n",
    "Xtest = rgb2lab(1.0/255*images[split:])[:,:,:,0]  # [:,:,:,0] -> select grayscale layer\n",
    "Xtest = Xtest.reshape(Xtest.shape+(1,))\n",
    "Ytest = rgb2lab(1.0/255*images[split:])[:,:,:,1:] # [:,:,:,1] -> select color layer\n",
    "\n",
    "# ab spectrum [-128,128] .. divide by 128 to get [-1,1]\n",
    "Ytest = Ytest / 128\n",
    "print(model.evaluate(Xtest, Ytest, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# colorizer section \n",
    "# init array and add img to color\n",
    "images_to_color = []\n",
    "for filename in os.listdir('Images_new/Test/'):\n",
    "    images_to_color.append(img_to_array(load_img('Images_new/Test/'+filename)))\n",
    "\n",
    "# covert color space, reshape img\n",
    "images_to_color = np.array(images_to_color, dtype=float)\n",
    "images_to_color = rgb2lab(1.0/255*images_to_color)[:,:,:,0]\n",
    "images_to_color = images_to_color.reshape(images_to_color.shape+(1,))\n",
    "\n",
    "# Test model with predict function\n",
    "output = model.predict(images_to_color)\n",
    "\n",
    "# covert color value back from [-1,1] to [-128,128] which is Lab color spectrum\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    \n",
    "    # make empty 256x256 with 3 layera\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    # copy grayscale layer to new obj\n",
    "    cur[:,:,0] = images_to_color[i][:,:,0]\n",
    "    # copy color layer to new obj\n",
    "    cur[:,:,1:] = output[i]\n",
    "    # save img in directory\n",
    "    imsave(\"Images_new/Result/img_\"+str(i)+\".png\", lab2rgb(cur))\n",
    "    #imsave(\"Images_new/Result/img_\"+str(i)+\".png\", img_as_ubyte(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
